WEBVTT

00:00:00.000 --> 00:00:11.030 align:middle line:84%
We propose a method to reconstruct, complete and semantically label a 3D scene from a single depth image based on GAN.

00:00:11.030 --> 00:00:30.030 align:middle line:84%
We use depth image as input and produce semantically labeled volumes. There are 12 categories in our experiments are based on SUNCG that includes: ceiling, floor, wall, window and so on.

00:00:30.030 --> 00:01:04.030 align:middle line:84%
The encoder for the given depth image compresses the input 2D image into a feature in the latent space. Its architecture is a concatenated network that sequentially combines 2D convolutional layers and max-pooling layers. The output of the encoder represents the latent feature of the semantic reconstruction architecture according to the given depth image.

00:01:04.030 --> 00:01:18.030 align:middle line:84%
Then the generator unwraps the latent feature to a higher dimensional voxel data. The output of the generator is the voxel-wise classification for 12 categories represented in multiple channels.

00:01:18.030 --> 00:01:41.030 align:middle line:84%
In summary, from the depth image to the 3D volume, our architecture is a concatenated structure of an encoder with 2D convolutional operators that convert the input depth image into a lower dimensional latent feature and a generator built with several 3D deconvolutional operators.

00:01:41.030 --> 00:01:57.030 align:middle line:84%
If we train this architecture directly, small objects can not be represented well, only common components could be predicted such as ceiling and floor.

00:01:57.030 --> 00:02:07.030 align:middle line:84%
The wall and door are sparsely reconstructed, The sofa and chairs are even unrecognisable.

00:02:07.030 --> 00:02:22.030 align:middle line:84%
Consider for the whole architecture which is concatenated with encoder and decoder. The latent feature is an important bridge between the given depth image and the expected semantic volumetric data.

00:02:22.030 --> 00:02:42.030 align:middle line:84%
Since reconstructing from one image has a restrictive view of the scene, we want to make the latent features of the depth image to be similar to the feature extracted from complete volumetric data.

00:02:42.030 --> 00:02:51.030 align:middle line:84%
We introduce discriminator D-l aiming at distinguishing the latent descriptors from each others.

00:02:51.030 --> 00:03:04.030 align:middle line:84%
The output of both encoders are passed to the same generator, the resulting latent feature of depth image is similar to the feature of the semantic volumetric data.

00:03:04.030 --> 00:03:17.030 align:middle line:84%
With the help of latent discriminator, our inference architecture can provide more accurate semantic predictions for small objects with sharper edges.

00:03:17.030 --> 00:03:30.030 align:middle line:84%
Based on these voxel representations, we can clearly visualize the superiority of our algorithm to reconstruct more detailed structures.

00:03:30.030 --> 00:03:35.030 align:middle line:84%
Our method performs better than 3D VAE and 3D-RecGAN++ which are the recent works on 3D generative model used for generating 3D volumetric data.