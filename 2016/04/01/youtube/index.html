<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Zigzagnet- Efficient deep learning for real object recognition based on 3D models - Yida Wang</title>
  <meta name="author" content="Yida Wang">
  <meta name="description" content="A blog about everything.">

  <meta name="generator" content="Hugo 0.68.3" />

  
  
  
  
  
  <link rel="stylesheet" href="/assets/css/external.min.css" media="screen">

  
  
  <link rel="stylesheet" href="/assets/css/styles.css" media="screen">

  <link href="//fonts.googleapis.com/css?family=Roboto:400" rel="stylesheet">

  
  
  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/src/images/apple-touch-icon.png">
  <link rel="shortcut icon" href="/src/images/favicon.ico">

  
  <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Yida Wang" />
  <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Yida Wang" />

  <meta property="og:title" content="Zigzagnet- Efficient deep learning for real object recognition based on 3D models" />
<meta property="og:description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2016zigzagnet, title={Zigzagnet: Efficient deep learning for real object recognition based on 3D models}, author={Wang, Yida and Cui, Can and Zhou, Xiuzhuang and Deng, Weihong}, booktitle={Asian Conference on Computer Vision}, pages={456--471}, year={2016}, organization={Springer} }	Abstrarct Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/2016/04/01/youtube/" />
<meta property="article:published_time" content="2016-04-01T10:15:01+02:00" />
<meta property="article:modified_time" content="2016-04-01T10:15:01+02:00" />

  
  <meta itemprop="name" content="Zigzagnet- Efficient deep learning for real object recognition based on 3D models">
<meta itemprop="description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2016zigzagnet, title={Zigzagnet: Efficient deep learning for real object recognition based on 3D models}, author={Wang, Yida and Cui, Can and Zhou, Xiuzhuang and Deng, Weihong}, booktitle={Asian Conference on Computer Vision}, pages={456--471}, year={2016}, organization={Springer} }	Abstrarct Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models.">
<meta itemprop="datePublished" content="2016-04-01T10:15:01&#43;02:00" />
<meta itemprop="dateModified" content="2016-04-01T10:15:01&#43;02:00" />
<meta itemprop="wordCount" content="276">



<meta itemprop="keywords" content="machine learning,computer vision," />
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Zigzagnet- Efficient deep learning for real object recognition based on 3D models"/>
<meta name="twitter:description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2016zigzagnet, title={Zigzagnet: Efficient deep learning for real object recognition based on 3D models}, author={Wang, Yida and Cui, Can and Zhou, Xiuzhuang and Deng, Weihong}, booktitle={Asian Conference on Computer Vision}, pages={456--471}, year={2016}, organization={Springer} }	Abstrarct Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models."/>

</head>

  <body>
    
      <nav>
  <a href="/" title="Homepage">
    Simplicity
  </a>
</nav>

    
    <main class="container">
      
  <article>
    <header>
      <small>
        <time datetime="2016-04-01 10:15:01&#43;0200">2016-04-01</time>
          &bull;
          
            
            <a href="/categories/publication">PUBLICATION</a>
          </small>
      <h1>Zigzagnet- Efficient deep learning for real object recognition based on 3D models</h1>
    </header>
    <section><p>If you find this work useful in yourr research, please cite:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">@inproceedings<span class="o">{</span>wang2016zigzagnet,
  <span class="nv">title</span><span class="o">={</span>Zigzagnet: Efficient deep learning <span class="k">for</span> real object recognition based on 3D models<span class="o">}</span>,
  <span class="nv">author</span><span class="o">={</span>Wang, Yida and Cui, Can and Zhou, Xiuzhuang and Deng, Weihong<span class="o">}</span>,
  <span class="nv">booktitle</span><span class="o">={</span>Asian Conference on Computer Vision<span class="o">}</span>,
  <span class="nv">pages</span><span class="o">={</span>456--471<span class="o">}</span>,
  <span class="nv">year</span><span class="o">={</span>2016<span class="o">}</span>,
  <span class="nv">organization</span><span class="o">={</span>Springer<span class="o">}</span>
<span class="o">}</span>	
</code></pre></div><h1 id="abstrarct">Abstrarct</h1>
<p>Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models. Network is initiated with multi-triplet cost function establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to pose, lighting condition, background and category information of rendered images. Fine-tuning with additional data further meets the aim of classification on special real photos based on initial model. We propose a 6.2 MB compact parametric model called ZigzagNet based on SqueezeNet to improve the performance for recognition by applying moving normalization inside micro architecture and adding channel wise convolutional bypass through macro architecture.  Moving batch normalization is used to get a good performance on both convergence speed and recognition accuracy. Accuracy of our compact parametric model in experiment on ImageNet and PASCAL samples provided by PASCAL3D+ based on simple Nearest Neighbor classifier is close to the result of 240 MB AlexNet trained with real images. Model trained on texture-less models which consumes less time for rendering and collecting outperforms the result of training with more textured models from ShapeNet.</p>
</section>
    <footer>
      <hr>
      <div class="meta">
        <p class="tags">
          
            
              <a href="/tags/machine-learning">
                <span>#</span>machine learning</a>
            
              <a href="/tags/computer-vision">
                <span>#</span>computer vision</a>
            
          
        </p>
      </div>
      <hr>
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "test" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </footer>
  </article>

    </main>
    
      <footer>
  <p>
    &copy; 2020 Yida Wang. <a href="http://creativecommons.org/licenses/by-sa/4.0/">Some Rights Reserved</a>.
  </p>
  <p>
    Powered by <a href="https://gohugo.io" title="A Fast and Flexible Website Generator">Hugo</a> &amp; <a href="https://github.com/eshlox/simplicity" title="Hugo theme">Simplicity</a>.
  </p>
</footer>

    
    
    
    
    
    
    
    <script src="/assets/js/scripts.min.js"></script>
    
  </body>
</html>
