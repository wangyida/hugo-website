<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image - Yida Wang</title>
  <meta name="author" content="Yida Wang">
  <meta name="description" content="A blog about everything.">

  <meta name="generator" content="Hugo 0.68.3" />

  
  
  
  
  
  <link rel="stylesheet" href="/assets/css/external.min.css" media="screen">

  
  
  <link rel="stylesheet" href="/assets/css/styles.css" media="screen">

  <link href="//fonts.googleapis.com/css?family=Roboto:400" rel="stylesheet">

  
  
  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/src/images/apple-touch-icon.png">
  <link rel="shortcut icon" href="/src/images/favicon.ico">

  
  <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Yida Wang" />
  <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Yida Wang" />

  <meta property="og:title" content="ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image" />
<meta property="og:description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2019forknet, title={ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image}, author={Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8608--8617}, year={2019} } Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/2019/11/01/youtube/" />
<meta property="article:published_time" content="2019-11-01T10:15:01+02:00" />
<meta property="article:modified_time" content="2019-11-01T10:15:01+02:00" />

  
  <meta itemprop="name" content="ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image">
<meta itemprop="description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2019forknet, title={ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image}, author={Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8608--8617}, year={2019} } Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space.">
<meta itemprop="datePublished" content="2019-11-01T10:15:01&#43;02:00" />
<meta itemprop="dateModified" content="2019-11-01T10:15:01&#43;02:00" />
<meta itemprop="wordCount" content="221">



<meta itemprop="keywords" content="machine learning,demo,computer vision," />
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image"/>
<meta name="twitter:description" content="If you find this work useful in yourr research, please cite:
@inproceedings{wang2019forknet, title={ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image}, author={Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico}, booktitle={Proceedings of the IEEE International Conference on Computer Vision}, pages={8608--8617}, year={2019} } Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space."/>

</head>

  <body>
    
      <nav>
  <a href="/" title="Homepage">
    Simplicity
  </a>
</nav>

    
    <main class="container">
      
  <article>
    <header>
      <small>
        <time datetime="2019-11-01 10:15:01&#43;0200">2019-11-01</time>
          &bull;
          
            
            <a href="/categories/publication">PUBLICATION</a>
          </small>
      <h1>ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image</h1>
    </header>
    <section><div class="embed-container">
  <iframe src="https://www.youtube.com/embed/1WZ16bGff1o" frameborder="0" allowfullscreen></iframe>
</div>

<p>If you find this work useful in yourr research, please cite:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">@inproceedings<span class="o">{</span>wang2019forknet,
  <span class="nv">title</span><span class="o">={</span>ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image<span class="o">}</span>,
  <span class="nv">author</span><span class="o">={</span>Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico<span class="o">}</span>,
  <span class="nv">booktitle</span><span class="o">={</span>Proceedings of the IEEE International Conference on Computer Vision<span class="o">}</span>,
  <span class="nv">pages</span><span class="o">={</span>8608--8617<span class="o">}</span>,
  <span class="nv">year</span><span class="o">={</span>2019<span class="o">}</span>
<span class="o">}</span>
</code></pre></div><h1 id="abstrarct">Abstrarct</h1>
<p>We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers.  Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion.</p>
</section>
    <footer>
      <hr>
      <div class="meta">
        <p class="tags">
          
            
              <a href="/tags/machine-learning">
                <span>#</span>machine learning</a>
            
              <a href="/tags/demo">
                <span>#</span>demo</a>
            
              <a href="/tags/computer-vision">
                <span>#</span>computer vision</a>
            
          
        </p>
      </div>
      <hr>
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "test" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </footer>
  </article>

    </main>
    
      <footer>
  <p>
    &copy; 2020 Yida Wang. <a href="http://creativecommons.org/licenses/by-sa/4.0/">Some Rights Reserved</a>.
  </p>
  <p>
    Powered by <a href="https://gohugo.io" title="A Fast and Flexible Website Generator">Hugo</a> &amp; <a href="https://github.com/eshlox/simplicity" title="Hugo theme">Simplicity</a>.
  </p>
</footer>

    
    
    
    
    
    
    
    <script src="/assets/js/scripts.min.js"></script>
    
  </body>
</html>
